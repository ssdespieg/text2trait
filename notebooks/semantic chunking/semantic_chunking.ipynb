{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "###############################################################################\n",
        "# CONFIGURE GOOGLE DRIVE MOUNTING AND SPACY SETUP\n",
        "###############################################################################\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download and load spaCy model\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "spacy.require_gpu()\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "###############################################################################\n",
        "# CHECKPOINT RECONSTRUCTION FROM PREVIOUSLY PROCESSED DATA\n",
        "###############################################################################\n",
        "\n",
        "# Set paths for output and checkpoint files\n",
        "output_file = \"/content/drive/MyDrive/COPENHAGEN/MSC THESIS/datasets/essays/full_chunked_local_minima.json\"\n",
        "checkpoint_file = \"/content/drive/MyDrive/COPENHAGEN/MSC THESIS/datasets/essays/processed_authids.txt\"\n",
        "\n",
        "# Reconstruct processed AUTHIDs from output file\n",
        "processed_authids = set()\n",
        "with open(output_file, 'r') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            chunk = json.loads(line)\n",
        "            processed_authids.add(chunk['#AUTHID'])\n",
        "\n",
        "# Save reconstructed AUTHIDs to checkpoint file\n",
        "with open(checkpoint_file, 'w') as cf:\n",
        "    for auth_id in processed_authids:\n",
        "        cf.write(auth_id + '\\n')\n",
        "\n",
        "print(\"Checkpoint reconstructed. Processed AUTHIDs have been saved to processed_authids.txt.\")\n",
        "\n",
        "###############################################################################\n",
        "# LOAD HUGGING FACE MODEL AND TOKENIZER FOR EMBEDDING EXTRACTION\n",
        "###############################################################################\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load Hugging Face model and tokenizer\n",
        "model_name = \"dunzhang/stella_en_1.5B_v5\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Add sentence segmentation pipeline to spaCy\n",
        "nlp.add_pipe('sentencizer')\n",
        "\n",
        "###############################################################################\n",
        "# EMBEDDING AND SIMILARITY FUNCTIONS\n",
        "###############################################################################\n",
        "\n",
        "def get_batch_embeddings(sentences: List[str], batch_size: int = 16) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes embeddings for a batch of sentences using the preloaded Hugging Face model.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch = sentences[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between two vectors.\n",
        "    \"\"\"\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def find_boundaries(similarities: List[float]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Identifies local minima in the similarity scores to determine chunk boundaries.\n",
        "    \"\"\"\n",
        "    boundaries = []\n",
        "    for i in range(1, len(similarities) - 1):\n",
        "        if similarities[i] < similarities[i - 1] and similarities[i] < similarities[i + 1]:\n",
        "            boundaries.append(i + 1)\n",
        "    return boundaries\n",
        "\n",
        "###############################################################################\n",
        "# PASS 1: PROCESSING FUNCTION TO SPLIT TEXT INTO CHUNKS BASED ON LOCAL MINIMA\n",
        "###############################################################################\n",
        "\n",
        "def process_row(row: pd.Series) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Processes a single row of the input DataFrame, splitting the text into chunks based on semantic similarity.\n",
        "    \"\"\"\n",
        "    text = row['TEXT']\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    # Handle texts with one or no sentences\n",
        "    if len(sentences) <= 1:\n",
        "        return [{\n",
        "            'Chunk Number': 1,\n",
        "            '#AUTHID': row['#AUTHID'],\n",
        "            'TEXT': \" \".join(sentences),\n",
        "            'chunking_method': 'semantic_similarity_local_minima',\n",
        "            'model': 'stella-1.5B',\n",
        "            'run_time': time.time() - start_time,\n",
        "            'cEXT': row['cEXT'],\n",
        "            'cNEU': row['cNEU'],\n",
        "            'cAGR': row['cAGR'],\n",
        "            'cCON': row['cCON'],\n",
        "            'cOPN': row['cOPN'],\n",
        "        }]\n",
        "\n",
        "    # Generate embeddings in batches\n",
        "    embeddings = get_batch_embeddings(sentences)\n",
        "\n",
        "    # Compute similarity between consecutive sentences\n",
        "    similarities = [cosine_similarity(embeddings[i], embeddings[i + 1]) for i in range(len(embeddings) - 1)]\n",
        "    boundaries = find_boundaries(similarities)\n",
        "\n",
        "    # Create chunks\n",
        "    chunk_start_indices = [0] + boundaries + [len(sentences)]\n",
        "    chunks = []\n",
        "    for i in range(len(chunk_start_indices) - 1):\n",
        "        start_idx = chunk_start_indices[i]\n",
        "        end_idx = chunk_start_indices[i + 1]\n",
        "        chunk_sentences = sentences[start_idx:end_idx]\n",
        "        chunk_text = \" \".join(chunk_sentences)\n",
        "        chunks.append({\n",
        "            'Chunk Number': i + 1,\n",
        "            '#AUTHID': row['#AUTHID'],\n",
        "            'TEXT': chunk_text,\n",
        "            'chunking_method': 'semantic_similarity_local_minima',\n",
        "            'model': 'stella-1.5B',\n",
        "            'run_time': time.time() - start_time,\n",
        "            'cEXT': row['cEXT'],\n",
        "            'cNEU': row['cNEU'],\n",
        "            'cAGR': row['cAGR'],\n",
        "            'cCON': row['cCON'],\n",
        "            'cOPN': row['cOPN'],\n",
        "        })\n",
        "    return chunks\n",
        "\n",
        "###############################################################################\n",
        "# PASS 2: PROCESSING AND MERGING CHUNKS BASED ON COSINE SIMILARITY\n",
        "###############################################################################\n",
        "\n",
        "def process_and_merge_chunks(input_path: str, output_path: str, merge_threshold: float = 0.4):\n",
        "    \"\"\"\n",
        "    Processes and merges chunks based on semantic similarity.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_path, encoding=\"latin1\")\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, 'r') as cf:\n",
        "            completed_authids = {line.strip() for line in cf if line.strip()}\n",
        "    else:\n",
        "        completed_authids = set()\n",
        "\n",
        "    remaining_rows = df[~df['#AUTHID'].isin(completed_authids)]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        for row_chunks in tqdm(executor.map(process_row, [row for _, row in remaining_rows.iterrows()]),\n",
        "                               total=len(remaining_rows), desc=\"Processing rows\"):\n",
        "            with open(output_path, 'a') as f:\n",
        "                for chunk in row_chunks:\n",
        "                    f.write(json.dumps(chunk) + '\\n')\n",
        "            current_authid = row_chunks[0]['#AUTHID']\n",
        "            with open(checkpoint_file, 'a') as cf:\n",
        "                cf.write(current_authid + '\\n')\n",
        "\n",
        "    print(\"Chunks based on semantic content saved successfully!\")"
      ],
      "metadata": {
        "id": "Iuf8BfW3l4wo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}