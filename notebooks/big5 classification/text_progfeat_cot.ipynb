{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtKoXV4-DWjw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# WSL path to project directory\n",
        "wsl_path = \"/mnt/g/My Drive/Sophia/MSC thesis\"\n",
        "\n",
        "# Change current working directory to the WSL path\n",
        "os.chdir(wsl_path)\n",
        "\n",
        "# Print the current working directory to verify the change\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NvOjymFDWjx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import openai\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "###############################################################################\n",
        "# CONFIGURE OPENAI CLIENT FOR V1.0+ USAGE\n",
        "###############################################################################\n",
        "\n",
        "# API key:\n",
        "openai.api_key = \"OPENAI-API-KEY\"  # Replace with actual API key\n",
        "\n",
        "# Model name\n",
        "MODEL_NAME = \"gpt-4o-2024-11-20\"\n",
        "\n",
        "###############################################################################\n",
        "# TRAIT LIST AND PROMPT\n",
        "###############################################################################\n",
        "\n",
        "TRAIT_LIST = [\n",
        "    \"Openness to Experience\",\n",
        "    \"Conscientiousness\",\n",
        "    \"Extroversion\",\n",
        "    \"Agreeableness\",\n",
        "    \"Neuroticism\"\n",
        "]\n",
        "\n",
        "def load_programmatic_features(filepath: str) -> dict:\n",
        "    \"\"\"Loads programmatic features from a JSON file and creates a lookup dictionary.\"\"\"\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    features_lookup = {}\n",
        "    for item in data:\n",
        "        key = (item[\"#AUTHID\"], item[\"Chunk Number\"])\n",
        "        features_lookup[key] = item[\"features_text\"]\n",
        "    return features_lookup\n",
        "\n",
        "def build_prompt_for_all_traits(text_chunk: str, programmatic_features: str) -> str:\n",
        "    \"\"\"\n",
        "    Request classification of all Big Five traits in a single JSON response, including programmatic features.\n",
        "    \"\"\"\n",
        "    traits_str = \"\\n\".join(f\"- {t}\" for t in TRAIT_LIST)\n",
        "    # Escape curly braces\n",
        "    sanitized_text_chunk = text_chunk.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "    sanitized_programmatic_features = programmatic_features.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "\n",
        "    prompt = f\"\"\"You are an intelligent and disciplined assistant trained to determine\n",
        "    the presence or absence of each of the Big Five personality traits in\n",
        "    a stream-of-consciousness text. The traits are:\n",
        "    {traits_str}\n",
        "\n",
        "    **You will be provided with the text and programmatic features extracted from the text. Consider these inputs in your analysis.**\n",
        "\n",
        "    **Your task is to reason through each trait step-by-step, explaining how the text and the relevant programmatic features provide evidence (or lack thereof) for each trait before determining the final result and confidence score.**\n",
        "\n",
        "    **Your output must be valid JSON** with the structure:\n",
        "\n",
        "    {{\n",
        "      \"traits\": [\n",
        "        {{\n",
        "          \"trait\": \"Openness to Experience\",\n",
        "          \"reasoning_steps\": [\n",
        "            \"...\"\n",
        "          ],\n",
        "          \"result\": \"y|n|NaN\",\n",
        "          \"result_justification\": \"...\",\n",
        "          \"confidence_score\": 0.0,\n",
        "          \"confidence_score_justification\": \"...\"\n",
        "        }},\n",
        "        {{\n",
        "          \"trait\": \"Conscientiousness\",\n",
        "          \"reasoning_steps\": [\n",
        "            \"...\"\n",
        "          ],\n",
        "          \"result\": \"y|n|NaN\",\n",
        "          \"result_justification\": \"...\",\n",
        "          \"confidence_score\": 0.0,\n",
        "          \"confidence_score_justification\": \"...\"\n",
        "        }},\n",
        "        ...\n",
        "      ]\n",
        "    }}\n",
        "\n",
        "    Rules:\n",
        "    1. For each trait, reason step-by-step as described, then provide the final evaluation in the JSON format.\n",
        "    2. Consider the provided programmatic features in your analysis and reasoning.\n",
        "    3. If insufficient data, set 'result'='NaN', 'result_justification'='NaN',\n",
        "       'confidence_score'=0.0, 'confidence_score_justification'='NaN'.\n",
        "    4. No extra text, no code fences, no keys beyond what is shown.\n",
        "    5. The 'result' MUST be 'y', 'n', or 'NaN'.\n",
        "    6. Provide detailed justifications for the reasoning, results, and confidence scores.\n",
        "\n",
        "    The text: {sanitized_text_chunk}\n",
        "\n",
        "    The programmatic features: {sanitized_programmatic_features}\n",
        "    \"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def classify_text_chunk(text_chunk: str, programmatic_features: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls the new v1.0+ method openai.chat.completions.create\n",
        "    with response_format for structured JSON.\n",
        "    \"\"\"\n",
        "    user_prompt = build_prompt_for_all_traits(text_chunk, programmatic_features)\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful, disciplined assistant that outputs JSON only.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_prompt\n",
        "            },\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=1.0,\n",
        "        max_tokens=2000\n",
        "    )\n",
        "    # Extract the JSON from the model\n",
        "    raw_json_str = response.choices[0].message.content\n",
        "    return raw_json_str\n",
        "\n",
        "###############################################################################\n",
        "# MAIN LOGIC: READ LINES, CALL MODEL, SAVE RESULTS INCREMENTALLY\n",
        "###############################################################################\n",
        "\n",
        "def main():\n",
        "    input_json = \"full_chunked_local_minima_pass_2_0.40.json\"\n",
        "    programmatic_features_file = \"full_programmatic_features_extracted_updated_2.json\"\n",
        "    output_file = \"cot_prog_features_big5-4o_temp_1.0.json\"\n",
        "    total_lines_to_process = 2000\n",
        "\n",
        "    if not os.path.exists(input_json):\n",
        "        print(f\"[ERROR] Input file does not exist: {input_json}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(programmatic_features_file):\n",
        "        print(f\"[ERROR] Programmatic features file does not exist: {programmatic_features_file}\")\n",
        "        return\n",
        "\n",
        "    # Load programmatic features\n",
        "    programmatic_features_lookup = load_programmatic_features(programmatic_features_file)\n",
        "\n",
        "    # --- Check processed lines at the start of the script ---\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            processed_count = sum(1 for _ in f)\n",
        "        print(f\"[INFO] Found {processed_count} lines already processed.\")\n",
        "    else:\n",
        "        processed_count = 0\n",
        "\n",
        "    # Store results line-by-line\n",
        "    with open(output_file, \"a\", encoding=\"utf-8\") as out_f:  # Open in append mode\n",
        "        with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            # Create a tqdm progress bar that will process up to `total_lines_to_process`\n",
        "            with tqdm(total=total_lines_to_process, desc=\"Processing Chunks\", initial=processed_count) as pbar:\n",
        "                for i, line in enumerate(f):\n",
        "                    # Process lines only if they haven't been processed already\n",
        "                    if i >= processed_count:\n",
        "                        row = json.loads(line)\n",
        "                        author_id = row.get(\"#AUTHID\", \"\")\n",
        "                        chunk_number = row.get(\"Chunk Number\", \"\")\n",
        "                        text_chunk = row.get(\"TEXT\", \"\")\n",
        "\n",
        "                        print(f\"\\n[PROCESSING] Row {i} => Author: {author_id}, Chunk: {chunk_number}\")\n",
        "\n",
        "                        # Get programmatic features for this chunk\n",
        "                        prog_features_key = (author_id, chunk_number)\n",
        "                        programmatic_features = programmatic_features_lookup.get(prog_features_key, \"No programmatic features found.\")\n",
        "\n",
        "                        try:\n",
        "                            raw_json = classify_text_chunk(text_chunk, programmatic_features)\n",
        "                            print(\"[RAW MODEL OUTPUT]\\n\", raw_json)\n",
        "\n",
        "                            # Parse the JSON to verify it's valid\n",
        "                            try:\n",
        "                                parsed = json.loads(raw_json)\n",
        "                            except json.JSONDecodeError as e:\n",
        "                                print(f\"[ERROR] Could not parse JSON for row {i}: {e}\")\n",
        "                                parsed = {\"error\": \"Invalid JSON\", \"exception\": str(e)}\n",
        "\n",
        "                            # Save the final record\n",
        "                            record = {\n",
        "                                \"author_id\": author_id,\n",
        "                                \"chunk_number\": chunk_number,\n",
        "                                \"model_output\": parsed\n",
        "                            }\n",
        "                            out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "                            processed_count += 1\n",
        "                            pbar.update(1)  # Update the progress bar\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERROR] In call to model: {e}\")\n",
        "                            record = {\n",
        "                                \"author_id\": author_id,\n",
        "                                \"chunk_number\": chunk_number,\n",
        "                                \"model_output\": {\"error\": str(e)}\n",
        "                            }\n",
        "                            out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "                    # Exit loop if total lines to process is reached\n",
        "                    if processed_count >= total_lines_to_process:\n",
        "                        break\n",
        "\n",
        "    print(f\"\\n[DONE] Wrote results to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}