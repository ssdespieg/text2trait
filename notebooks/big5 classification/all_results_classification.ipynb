{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YJEXSrHpyiR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# WSL path to project directory\n",
        "wsl_path = \"/content/drive/My Drive/MSC thesis/final_datasets/big5_classification\"\n",
        "\n",
        "# Change current working directory to the WSL path\n",
        "os.chdir(wsl_path)\n",
        "\n",
        "# Print the current working directory to verify the change\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Big5 Personality Analysis Pipeline and Visualization\n",
        "\n",
        "This script processes and analyzes Big5 personality assessment results and generates\n",
        "visualizations for different analytical perspectives:\n",
        "1. Combined results processing and analysis\n",
        "2. NaN occurrence patterns analysis\n",
        "3. Statistical analysis with ground truth\n",
        "4. Visualization of accuracy metrics:\n",
        "   - Overall accuracy by combined condition\n",
        "   - Trait-level accuracy by condition\n",
        "   - Accuracy by prompting technique\n",
        "   - Accuracy by input type\n",
        "   - Author-level aggregated analysis\n",
        "\n",
        "This script processes and analyzes Big5 personality assessment results from various conditions:\n",
        "- Text-only baseline and chain-of-thought (CoT)\n",
        "- Programmatic features baseline and CoT\n",
        "- Semantic features baseline and CoT\n",
        "\n",
        "The script performs three main functions:\n",
        "1. Combines results from different conditions into a single dataset\n",
        "2. Analyzes NaN occurrence patterns across conditions\n",
        "3. Performs statistical analysis on the combined dataset\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File paths configuration\n",
        "PATHS = {\n",
        "    \"text_only_baseline\": \"big5_text_only_classification/text_only_baseline_big5_results_gpt-4o_temp_1.0.json\",\n",
        "    \"text_only_cot\": \"big5_text_only_classification/text_only_cot_big5_results_gpt-4o_temp_1.0.json\",\n",
        "    \"progfeat_baseline\": \"big5_programmatic_features_classification/progfeat_baseline_big5_openai_gpt-4o_temp_1.0.json\",\n",
        "    \"progfeat_cot\": \"big5_programmatic_features_classification/progfeat_cot_big5_openai_gpt-4o_temp_1.0.json\",\n",
        "    \"semantic_baseline\": \"big5_semantic_features_classification/semfeat_baseline_big5_openai_gpt-4o_temp_1.0.json\",\n",
        "    \"semantic_cot\": \"big5_semantic_features_classification/semfeat_cot_big5_openai_gpt-4o_temp_1.0.json\",\n",
        "    \"ground_truth\": \"../full_chunked_local_minima_pass_2_0.40.json\"\n",
        "}\n",
        "\n",
        "def load_jsonl_to_records(file_path, prompt_type, input_type):\n",
        "    \"\"\"\n",
        "    Load and process a JSONL file into a list of standardized records.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the JSONL file\n",
        "        prompt_type (str): Type of prompt ('baseline' or 'cot')\n",
        "        input_type (str): Type of input ('text_only', 'text_programmatic', or 'text_semantic')\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries containing processed records\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            data = json.loads(line)\n",
        "            record = {\n",
        "                \"task\": \"big5\",\n",
        "                \"model\": \"gpt-4o\",\n",
        "                \"prompt\": prompt_type,\n",
        "                \"input\": input_type,\n",
        "                \"author_id\": data[\"author_id\"],\n",
        "                \"chunk_number\": data[\"chunk_number\"],\n",
        "                \"model_output\": data[\"model_output\"]\n",
        "            }\n",
        "            records.append(record)\n",
        "    return records\n",
        "\n",
        "def contains_nan(model_output_str):\n",
        "    \"\"\"\n",
        "    Check if a model output string contains any 'NaN' values.\n",
        "\n",
        "    Args:\n",
        "        model_output_str: String representation of model output JSON\n",
        "\n",
        "    Returns:\n",
        "        bool: True if NaN is present, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(model_output_str, str):\n",
        "            model_output = json.loads(model_output_str.replace(\"'\", \"\\\"\"))\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "        return any(trait.get(\"result\") == \"NaN\" for trait in model_output.get(\"traits\", []))\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        return False\n",
        "\n",
        "def get_trait_nan_status(model_output_str):\n",
        "    \"\"\"\n",
        "    Extract NaN status for each Big5 trait from model output.\n",
        "\n",
        "    Args:\n",
        "        model_output_str: String representation of model output JSON\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing NaN status for each trait\n",
        "    \"\"\"\n",
        "    trait_nan_status = {\n",
        "        \"Openness to Experience\": False,\n",
        "        \"Conscientiousness\": False,\n",
        "        \"Extroversion\": False,\n",
        "        \"Agreeableness\": False,\n",
        "        \"Neuroticism\": False,\n",
        "    }\n",
        "    try:\n",
        "        if isinstance(model_output_str, str):\n",
        "            model_output = json.loads(model_output_str.replace(\"'\", \"\\\"\"))\n",
        "            for trait in model_output.get(\"traits\", []):\n",
        "                if trait.get(\"result\") == \"NaN\":\n",
        "                    trait_nan_status[trait.get(\"trait\")] = True\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        pass\n",
        "    return trait_nan_status\n",
        "\n",
        "def get_word_count(model_output_str):\n",
        "    \"\"\"\n",
        "    Calculate approximate word count from model output.\n",
        "\n",
        "    Args:\n",
        "        model_output_str: String representation of model output JSON\n",
        "\n",
        "    Returns:\n",
        "        int: Total word count\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(model_output_str, str):\n",
        "            model_output = json.loads(model_output_str.replace(\"'\", \"\\\"\"))\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "        word_count = 0\n",
        "        for trait in model_output.get(\"traits\", []):\n",
        "            # Count words in reasoning steps\n",
        "            word_count += sum(len(step.split()) for step in trait.get(\"reasoning_steps\", []))\n",
        "            # Count words in justifications\n",
        "            word_count += len(trait.get(\"result_justification\", \"\").split())\n",
        "            word_count += len(trait.get(\"confidence_score_justification\", \"\").split())\n",
        "\n",
        "        return word_count\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        return 0\n",
        "\n",
        "def create_visualizations(merged_df):\n",
        "    \"\"\"\n",
        "    Create and display visualizations for accuracy analysis.\n",
        "\n",
        "    Args:\n",
        "        merged_df: DataFrame containing the merged results\n",
        "    \"\"\"\n",
        "    # Style configurations\n",
        "    plt.style.use('seaborn')\n",
        "\n",
        "    # Color schemes\n",
        "    cond_colors = {\n",
        "        \"text_only-baseline\": \"#5DA786\",\n",
        "        \"text_only-cot\": \"#B5E3D8\",\n",
        "        \"text_programmatic-baseline\": \"#F3995F\",\n",
        "        \"text_programmatic-cot\": \"#FBD4B2\",\n",
        "        \"text_semantic-baseline\": \"#7B3294\",\n",
        "        \"text_semantic-cot\": \"#C2A5CF\",\n",
        "    }\n",
        "\n",
        "    condition_label_map = {\n",
        "        \"text_only-baseline\": \"Text Only : Baseline\",\n",
        "        \"text_only-cot\": \"Text Only : CoT\",\n",
        "        \"text_programmatic-baseline\": \"Text + Prog : Baseline\",\n",
        "        \"text_programmatic-cot\": \"Text + Prog : CoT\",\n",
        "        \"text_semantic-baseline\": \"Text + Sem : Baseline\",\n",
        "        \"text_semantic-cot\": \"Text + Sem : CoT\",\n",
        "    }\n",
        "\n",
        "    condition_order = [\n",
        "        \"text_only-baseline\",\n",
        "        \"text_only-cot\",\n",
        "        \"text_programmatic-baseline\",\n",
        "        \"text_programmatic-cot\",\n",
        "        \"text_semantic-baseline\",\n",
        "        \"text_semantic-cot\",\n",
        "    ]\n",
        "\n",
        "    # Trait columns\n",
        "    trait_cols = [\"cOPN_accuracy\", \"cCON_accuracy\", \"cEXT_accuracy\", \"cAGR_accuracy\", \"cNEU_accuracy\"]\n",
        "\n",
        "    # Overall accuracy visualization\n",
        "    create_overall_accuracy_plot(merged_df, trait_cols, cond_colors, condition_label_map, condition_order)\n",
        "\n",
        "    # Trait-level accuracy visualization\n",
        "    create_trait_level_plot(merged_df, trait_cols, cond_colors, condition_label_map, condition_order)\n",
        "\n",
        "    # Prompting technique visualization\n",
        "    create_prompt_technique_plot(merged_df, trait_cols)\n",
        "\n",
        "    # Input type visualization\n",
        "    create_input_type_plot(merged_df, trait_cols)\n",
        "\n",
        "    # Author-level aggregated analysis\n",
        "    create_author_level_analysis(merged_df)\n",
        "\n",
        "def create_overall_accuracy_plot(df, trait_cols, cond_colors, condition_label_map, condition_order):\n",
        "    \"\"\"Create and display the overall accuracy plot\"\"\"\n",
        "    grouped_overall = df.groupby([\"input\", \"prompt\"], as_index=False)[trait_cols].mean()\n",
        "    grouped_overall[\"Condition\"] = grouped_overall[\"input\"] + \"-\" + grouped_overall[\"prompt\"]\n",
        "    grouped_overall[\"MeanOverallAccuracy\"] = grouped_overall[trait_cols].mean(axis=1)\n",
        "\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    ax = sns.barplot(\n",
        "        data=grouped_overall,\n",
        "        x=\"Condition\",\n",
        "        y=\"MeanOverallAccuracy\",\n",
        "        palette=cond_colors,\n",
        "        order=condition_order,\n",
        "        alpha=0.9,\n",
        "        dodge=False,\n",
        "        width=0.4\n",
        "    )\n",
        "\n",
        "    format_accuracy_plot(ax, \"Overall Accuracy by Combined Condition\",\n",
        "                        condition_label_map, condition_order)\n",
        "    plt.show()\n",
        "\n",
        "def create_trait_level_plot(df, trait_cols, cond_colors, condition_label_map, condition_order):\n",
        "    \"\"\"Create and display the trait-level accuracy plot\"\"\"\n",
        "    grouped_traits = df.groupby([\"input\", \"prompt\"], as_index=False)[trait_cols].mean()\n",
        "    grouped_traits[\"Condition\"] = grouped_traits[\"input\"] + \"-\" + grouped_traits[\"prompt\"]\n",
        "\n",
        "    trait_name_map = {\n",
        "        \"cOPN_accuracy\": \"Openness to Experience\",\n",
        "        \"cCON_accuracy\": \"Conscientiousness\",\n",
        "        \"cEXT_accuracy\": \"Extroversion\",\n",
        "        \"cAGR_accuracy\": \"Agreeableness\",\n",
        "        \"cNEU_accuracy\": \"Neuroticism\",\n",
        "    }\n",
        "\n",
        "    melted_traits = format_trait_data(grouped_traits, trait_cols, trait_name_map)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ax = create_trait_plot(melted_traits, cond_colors, condition_order)\n",
        "    format_trait_plot(ax, condition_label_map)\n",
        "    plt.show()\n",
        "\n",
        "def create_prompt_technique_plot(df, trait_cols):\n",
        "    \"\"\"Create and display the prompting technique accuracy plot\"\"\"\n",
        "    grouped_prompt = df.groupby(\"prompt\", as_index=False)[trait_cols].mean()\n",
        "    grouped_prompt[\"OverallAccuracy\"] = grouped_prompt[trait_cols].mean(axis=1)\n",
        "\n",
        "    prompt_colors = {\n",
        "        \"baseline\": \"#5DA786\",\n",
        "        \"cot\": \"#B5E3D8\",\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    ax = create_prompt_plot(grouped_prompt, prompt_colors)\n",
        "    format_prompt_plot(ax)\n",
        "    plt.show()\n",
        "\n",
        "def create_input_type_plot(df, trait_cols):\n",
        "    \"\"\"Create and display the input type accuracy plot\"\"\"\n",
        "    grouped_input = df.groupby(\"input\", as_index=False)[trait_cols].mean()\n",
        "    grouped_input[\"OverallAccuracy\"] = grouped_input[trait_cols].mean(axis=1)\n",
        "\n",
        "    input_colors = {\n",
        "        \"text_only\": \"#5DA786\",\n",
        "        \"text_programmatic\": \"#F3995F\",\n",
        "        \"text_semantic\": \"#7B3294\",\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    ax = create_input_plot(grouped_input, input_colors)\n",
        "    format_input_plot(ax)\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # 1. Load and combine all conditions\n",
        "    all_records = []\n",
        "    conditions = [\n",
        "        (PATHS[\"text_only_baseline\"], \"baseline\", \"text_only\"),\n",
        "        (PATHS[\"text_only_cot\"], \"cot\", \"text_only\"),\n",
        "        (PATHS[\"progfeat_baseline\"], \"baseline\", \"text_programmatic\"),\n",
        "        (PATHS[\"progfeat_cot\"], \"cot\", \"text_programmatic\"),\n",
        "        (PATHS[\"semantic_baseline\"], \"baseline\", \"text_semantic\"),\n",
        "        (PATHS[\"semantic_cot\"], \"cot\", \"text_semantic\")\n",
        "    ]\n",
        "\n",
        "    for file_path, prompt_type, input_type in conditions:\n",
        "        all_records.extend(load_jsonl_to_records(file_path, prompt_type, input_type))\n",
        "\n",
        "    # Create combined DataFrame\n",
        "    big_df = pd.DataFrame(\n",
        "        all_records,\n",
        "        columns=[\"task\", \"model\", \"prompt\", \"input\", \"author_id\", \"chunk_number\", \"model_output\"]\n",
        "    )\n",
        "\n",
        "    # Save combined results\n",
        "    out_csv = \"big5_combined_results.csv\"\n",
        "    big_df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved combined DataFrame to {out_csv} with {len(big_df)} rows.\")\n",
        "\n",
        "    # 2. Analyze NaN patterns\n",
        "    big_df[\"has_nan\"] = big_df[\"model_output\"].apply(contains_nan)\n",
        "\n",
        "    # Overall NaN statistics\n",
        "    total_rows = len(big_df)\n",
        "    total_nans = big_df[\"has_nan\"].sum()\n",
        "    print(f\"\\nNaN Statistics:\")\n",
        "    print(f\"Total rows: {total_rows}\")\n",
        "    print(f\"Total rows with at least one NaN: {total_nans}\")\n",
        "    print(f\"Overall NaN rate: {total_nans / total_rows * 100:.2f}%\")\n",
        "\n",
        "    # NaN counts by condition\n",
        "    print(\"\\nNaN counts by prompt:\")\n",
        "    for prompt in big_df[\"prompt\"].unique():\n",
        "        prompt_df = big_df[big_df[\"prompt\"] == prompt]\n",
        "        num_nans = prompt_df[\"has_nan\"].sum()\n",
        "        print(f\"  {prompt}: {num_nans} (out of {len(prompt_df)}) - {num_nans / len(prompt_df) * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\nNaN counts by input type:\")\n",
        "    for input_type in big_df[\"input\"].unique():\n",
        "        input_df = big_df[big_df[\"input\"] == input_type]\n",
        "        num_nans = input_df[\"has_nan\"].sum()\n",
        "        print(f\"  {input_type}: {num_nans} (out of {len(input_df)}) - {num_nans / len(input_df) * 100:.2f}%\")\n",
        "\n",
        "    # Trait-specific NaN analysis\n",
        "    trait_nan_status_df = big_df[\"model_output\"].apply(get_trait_nan_status).apply(pd.Series)\n",
        "    big_df = pd.concat([big_df, trait_nan_status_df], axis=1)\n",
        "\n",
        "    print(\"\\nNaN counts by trait:\")\n",
        "    for trait in trait_nan_status_df.columns:\n",
        "        num_nans = big_df[trait].sum()\n",
        "        print(f\"  {trait}: {num_nans} (out of {len(big_df)}) - {num_nans / len(big_df) * 100:.2f}%\")\n",
        "\n",
        "    # 3. Statistical analysis with ground truth\n",
        "    # Load ground truth data\n",
        "    ground_rows = []\n",
        "    with open(PATHS[\"ground_truth\"], \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                ground_rows.append(json.loads(line))\n",
        "\n",
        "    gt_df = pd.DataFrame(ground_rows)\n",
        "    gt_df = gt_df.rename(columns={\"#AUTHID\": \"author_id\", \"Chunk Number\": \"chunk_number\"})\n",
        "\n",
        "    # Merge with ground truth\n",
        "    merged_df = pd.merge(big_df, gt_df, on=[\"author_id\", \"chunk_number\"], how=\"inner\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    num_authors = merged_df[\"author_id\"].nunique()\n",
        "    num_chunks = len(merged_df)\n",
        "    chunks_per_author = merged_df.groupby(\"author_id\")[\"chunk_number\"].count()\n",
        "    merged_df[\"word_count_actual\"] = merged_df[\"TEXT\"].str.split().str.len()\n",
        "\n",
        "    # Print statistical results\n",
        "    print(\"\\nDataset Statistics:\")\n",
        "    print(f\"Number of unique authors: {num_authors}\")\n",
        "    print(f\"Number of unique chunks: {num_chunks}\")\n",
        "    print(\"\\nChunks per author statistics:\")\n",
        "    print(chunks_per_author.describe())\n",
        "    print(\"\\nActual word count per chunk statistics:\")\n",
        "    print(merged_df[\"word_count_actual\"].describe())\n",
        "    print(\"\\nDistribution of prompt types:\")\n",
        "    print(merged_df[\"prompt\"].value_counts())\n",
        "    print(\"\\nDistribution of input types:\")\n",
        "    print(merged_df[\"input\"].value_counts())\n",
        "\n",
        "def create_author_level_analysis(df):\n",
        "    \"\"\"Create and display author-level aggregated analysis\"\"\"\n",
        "    def majority_vote(series):\n",
        "        \"\"\"Return the most frequent value or NaN if no valid prediction exists.\"\"\"\n",
        "        if series.mode().empty:\n",
        "            return \"NaN\"\n",
        "        return series.mode()[0]\n",
        "\n",
        "    # Add combined condition column\n",
        "    df[\"Condition\"] = df[\"input\"] + \" : \" + df[\"prompt\"]\n",
        "\n",
        "    # Aggregate to author level\n",
        "    author_level = aggregate_to_author_level(df, majority_vote)\n",
        "\n",
        "    # Calculate and visualize accuracy scores\n",
        "    condition_accuracy_df = calculate_condition_accuracy(author_level)\n",
        "\n",
        "    # Create visualizations\n",
        "    create_author_level_plots(condition_accuracy_df, author_level)\n",
        "\n",
        "def aggregate_to_author_level(df, majority_vote):\n",
        "    \"\"\"Aggregate chunk-level predictions to author-level\"\"\"\n",
        "    author_level_conditions = df.groupby([\"author_id\", \"Condition\"]).agg(\n",
        "        {f\"{trait}_pred\": majority_vote for trait in [\"cOPN\", \"cCON\", \"cEXT\", \"cAGR\", \"cNEU\"]}\n",
        "    ).reset_index()\n",
        "\n",
        "    ground_truth = df.groupby(\"author_id\").first()[[\"cOPN\", \"cCON\", \"cEXT\", \"cAGR\", \"cNEU\"]]\n",
        "    return author_level_conditions.merge(ground_truth, on=\"author_id\", how=\"inner\")\n",
        "\n",
        "def calculate_condition_accuracy(author_level):\n",
        "    \"\"\"Calculate accuracy scores per condition for each trait\"\"\"\n",
        "    condition_accuracy_scores = {}\n",
        "    for condition in author_level[\"Condition\"].unique():\n",
        "        condition_data = author_level[author_level[\"Condition\"] == condition]\n",
        "        accuracy_scores = calculate_trait_accuracy(condition_data)\n",
        "        condition_accuracy_scores[condition] = accuracy_scores\n",
        "\n",
        "    return format_condition_accuracy(condition_accuracy_scores)\n",
        "\n",
        "def create_author_level_plots(condition_accuracy_df, author_level):\n",
        "    \"\"\"Create and display author-level visualizations\"\"\"\n",
        "    cond_colors = {\n",
        "        \"text_only : baseline\": \"#5DA786\",\n",
        "        \"text_only : cot\": \"#B5E3D8\",\n",
        "        \"text_programmatic : baseline\": \"#F3995F\",\n",
        "        \"text_programmatic : cot\": \"#FBD4B2\",\n",
        "        \"text_semantic : baseline\": \"#7B3294\",\n",
        "        \"text_semantic : cot\": \"#C2A5CF\",\n",
        "    }\n",
        "\n",
        "    condition_label_map = {\n",
        "        \"text_only : baseline\": \"Text Only : Baseline\",\n",
        "        \"text_only : cot\": \"Text Only : CoT\",\n",
        "        \"text_programmatic : baseline\": \"Text + Prog : Baseline\",\n",
        "        \"text_programmatic : cot\": \"Text + Prog : CoT\",\n",
        "        \"text_semantic : baseline\": \"Text + Sem : Baseline\",\n",
        "        \"text_semantic : cot\": \"Text + Sem : CoT\",\n",
        "    }\n",
        "\n",
        "    create_author_overall_plot(condition_accuracy_df, cond_colors, condition_label_map, author_level)\n",
        "    create_author_trait_plot(condition_accuracy_df, cond_colors, condition_label_map, author_level)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "k3v6BynDy8UK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "r2VsLDvDGA7p",
        "63rs0sPE3Lpf"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}