{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This script performs a comprehensive analysis of a semantically chunked stream-of-consciousness text dataset \n",
        "to gain insights into its structure and characteristics. The analysis focuses on chunk-level statistics, \n",
        "identifying outliers, visualizing key metrics, and extracting programmatic features that can be used \n",
        "for personality classification tasks.\n",
        "\n",
        "The script is divided into several sections:\n",
        "\n",
        "1. Exploratory Data Analysis (EDA):\n",
        "   - Calculates basic descriptive statistics, including word count, character count, and chunk distribution across authors.\n",
        "   - Identifies outliers in word count using the Interquartile Range (IQR) method and filters them out for further analysis.\n",
        "   - Visualizes the distribution of word counts, chunk counts per author, and sentiment scores.\n",
        "\n",
        "2. Text Preprocessing:\n",
        "   - Cleans the text by removing stopwords and punctuation, and applies stemming using the Porter Stemmer.\n",
        "   - Computes lexical diversity metrics, including Type-Token Ratio (TTR) and Measure of Textual Lexical Diversity (MTLD).\n",
        "\n",
        "3. Sentiment Analysis:\n",
        "   - Uses the VADER Sentiment Intensity Analyzer to calculate sentiment scores for each text chunk.\n",
        "\n",
        "4. Programmatic Feature Extraction:\n",
        "   - Extracts various features from the text using SpaCy and NLTK. These features include:\n",
        "     - Word and character counts\n",
        "     - Lexical diversity (TTR and MTLD)\n",
        "     - Sentiment polarity\n",
        "     - Personal pronoun usage\n",
        "     - Part-of-speech ratios (adverbs, pronouns, verbs, nouns)\n",
        "     - Tense distribution (past, present, future)\n",
        "\n",
        "5. Feature Export:\n",
        "   - Formats the extracted features into a structured JSON file for use in the personality classification experiment.\n",
        "\n",
        "\"\"\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws8ZIEUeVxAx"
      },
      "source": [
        "##Chunks EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbdNkJzvrKYP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# --- Load Dataset ---\n",
        "df_real = pd.read_json('/path/to/full_chunked_local_minima_pass_2_0.40.json', orient='records', lines=True)\n",
        "\n",
        "# --- Descriptive Statistics ---\n",
        "df_real['word_count'] = df_real['TEXT'].str.split().str.len()\n",
        "df_real['char_count'] = df_real['TEXT'].str.len()\n",
        "\n",
        "chunks_per_author = df_real.groupby('#AUTHID').size()\n",
        "\n",
        "# Outlier thresholds using IQR\n",
        "Q1, Q3 = df_real['word_count'].quantile([0.25, 0.75])\n",
        "IQR = Q3 - Q1\n",
        "lower_bound, upper_bound = Q1 - 3 * IQR, Q3 + 3 * IQR\n",
        "\n",
        "# Filter out extreme outliers\n",
        "df_no_extreme = df_real[(df_real['word_count'] >= lower_bound) & (df_real['word_count'] <= upper_bound)]\n",
        "\n",
        "# --- Summary Statistics ---\n",
        "print(\"=== Chunk Statistics Analysis ===\")\n",
        "print(f\"Total Chunks: {len(df_real)}\")\n",
        "print(f\"Unique Authors: {df_real['#AUTHID'].nunique()}\")\n",
        "print(f\"Average Words per Chunk (no extreme outliers): {df_no_extreme['word_count'].mean():.2f}\")\n",
        "print(f\"Median Words per Chunk: {df_no_extreme['word_count'].median():.2f}\")\n",
        "print(f\"Standard Deviation: {df_no_extreme['word_count'].std():.2f}\")\n",
        "\n",
        "# --- Outlier Information ---\n",
        "word_count_outliers = df_real[(df_real['word_count'] < lower_bound) | (df_real['word_count'] > upper_bound)]\n",
        "print(f\"\\nOutliers Detected: {len(word_count_outliers)}\")\n",
        "for _, row in word_count_outliers.iterrows():\n",
        "    print(f\"AuthorID: {row['#AUTHID']}, Chunk Number: {row['Chunk Number']}, Words: {row['word_count']}\")\n",
        "\n",
        "# --- Visualizations ---\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['axes.grid'] = False\n",
        "\n",
        "# Word Count Distribution (Without Outliers)\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.histplot(data=df_no_extreme['word_count'], kde=True, bins=50, color=\"skyblue\", ax=ax)\n",
        "ax.axvline(df_no_extreme['word_count'].mean(), color='navy', linestyle='--', label='Mean')\n",
        "ax.set_title(\"Word Count Distribution (Without Outliers)\")\n",
        "ax.set_xlabel(\"Words per Chunk\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# Chunks per Author Distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.histplot(data=chunks_per_author, kde=True, bins=30, color=\"skyblue\", ax=ax)\n",
        "ax.axvline(chunks_per_author.mean(), color='navy', linestyle='--', label='Mean')\n",
        "ax.set_title(\"Chunks per Author Distribution\")\n",
        "ax.set_xlabel(\"Chunks\")\n",
        "ax.set_ylabel(\"Authors\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = [stemmer.stem(word.strip(string.punctuation)) for word in text.split() if word.lower() not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df_real['cleaned_text'] = df_real['TEXT'].apply(preprocess_text)\n",
        "\n",
        "# --- Lexical Diversity ---\n",
        "def calculate_ttr(text):\n",
        "    words = text.split()\n",
        "    return len(set(words)) / len(words) if words else 0\n",
        "\n",
        "def calculate_mtld(text, threshold=0.72):\n",
        "    words = text.split()\n",
        "    segments, segment, unique_words = [], [], set()\n",
        "    for word in words:\n",
        "        segment.append(word)\n",
        "        unique_words.add(word)\n",
        "        if len(unique_words) / len(segment) < threshold:\n",
        "            segments.append(len(segment))\n",
        "            segment, unique_words = [], set()\n",
        "    return np.mean(segments) if segments else 0\n",
        "\n",
        "df_real['TTR'] = df_real['cleaned_text'].apply(calculate_ttr)\n",
        "df_real['MTLD'] = df_real['cleaned_text'].apply(calculate_mtld)\n",
        "\n",
        "# --- Sentiment Analysis ---\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "df_real['sentiment_score'] = df_real['cleaned_text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
        "\n",
        "# Sentiment Score Distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.histplot(data=df_real['sentiment_score'], kde=True, color=\"skyblue\", bins=30, ax=ax)\n",
        "ax.set_title(\"Distribution of Sentiment Scores\")\n",
        "ax.set_xlabel(\"Sentiment Score\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# --- Word Cloud ---\n",
        "def generate_word_cloud(text, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Generate Word Cloud\n",
        "all_text = \" \".join(df_real['cleaned_text'])\n",
        "generate_word_cloud(all_text, \"Word Cloud for All Text\")\n",
        "\n",
        "# --- Export Features ---\n",
        "export_path = \"/path/to/full_programmatic_features_extracted.json\"\n",
        "df_real.to_json(export_path, orient='records', lines=True)\n",
        "print(f\"Features exported to {export_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "He94Mr3g1Zv6"
      },
      "outputs": [],
      "source": [
        "# Set style parameters\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['axes.grid'] = False\n",
        "\n",
        "# Create combination column for all traits\n",
        "df_real['combination'] = df_real[['cOPN', 'cCON', 'cEXT', 'cAGR', 'cNEU']].agg(''.join, axis=1)\n",
        "\n",
        "# Get combination counts\n",
        "combination_counts = df_real['combination'].value_counts()\n",
        "\n",
        "# Create a mapping dictionary for trait names\n",
        "trait_name_map = {\n",
        "    'cOPN': 'Openness to Experience',\n",
        "    'cCON': 'Conscientiousness',\n",
        "    'cEXT': 'Extraversion',\n",
        "    'cAGR': 'Agreeableness',\n",
        "    'cNEU': 'Neuroticism',\n",
        "}\n",
        "\n",
        "personality_traits = ['cOPN', 'cCON', 'cEXT', 'cAGR', 'cNEU']\n",
        "trait_proportions = pd.DataFrame()\n",
        "\n",
        "# Calculate proportions for each trait\n",
        "for trait in personality_traits:\n",
        "    counts = df_real[trait].value_counts(normalize=True)\n",
        "    trait_proportions[trait_name_map[trait]] = counts\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "# Plot 1: Distribution of Personality Trait Combinations\n",
        "ax1 = axes[0]\n",
        "combination_counts.plot(kind='bar', ax=ax1, color='mediumaquamarine', alpha=1.0)\n",
        "ax1.set_title('Distribution of Personality Trait Combinations', fontsize=14)\n",
        "ax1.set_xlabel('Trait Combinations (cOPN, cCON, cEXT, cAGR, cNEU)')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.text(0.98, 0.95, f\"n = {len(df_real)}\", transform=ax1.transAxes, ha=\"right\", va=\"top\", fontsize=10,\n",
        "         bbox=dict(facecolor=\"white\", alpha=0.8, boxstyle=\"round\"))\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for i, v in enumerate(combination_counts):\n",
        "    ax1.text(i, v, str(v), ha='center', va='bottom')\n",
        "\n",
        "# Plot 2: Distribution of Binary Labels for Big Five Personality Traits\n",
        "ax2 = axes[1]\n",
        "trait_proportions.T.plot(kind='bar', stacked=True, ax=ax2,\n",
        "                         color=['lightcoral', 'lightgreen'], alpha=1.0)\n",
        "ax2.set_title('Distribution of Binary Labels for Big Five Personality Traits', fontsize=14)\n",
        "ax2.set_xlabel('Personality Trait')\n",
        "ax2.set_ylabel('Proportion')\n",
        "ax2.tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Adjust legend position and style\n",
        "ax2.legend(['No', 'Yes'], title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Add n = annotation\n",
        "ax2.text(0.98, 0.95, f\"n = {len(df_real)}\", transform=ax2.transAxes, ha=\"right\", va=\"top\", fontsize=10,\n",
        "         bbox=dict(facecolor=\"white\", alpha=0.8, boxstyle=\"round\"))\n",
        "\n",
        "# Add percentage labels\n",
        "for c in ax2.containers:\n",
        "    ax2.bar_label(c, fmt='%.2f', label_type='center')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print numerical summaries\n",
        "print(\"\\nIndividual Trait Distributions:\")\n",
        "for trait in personality_traits:\n",
        "    counts = df_real[trait].value_counts()\n",
        "    percentages = df_real[trait].value_counts(normalize=True) * 100\n",
        "    print(f\"\\n{trait}:\")\n",
        "    for label in counts.index:\n",
        "        print(f\"{label}: {counts[label]} ({percentages[label]:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52_2qSgzKkif"
      },
      "source": [
        "#Programatic Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYWmBm42IiW7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "# --- Load Dataset ---\n",
        "df_real = pd.read_json('/path/to/full_chunked_local_minima_pass_2_0.40.json', orient='records', lines=True)\n",
        "\n",
        "# --- Ensure necessary downloads and load models ---\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def extract_features(text):\n",
        "    text = clean_text(text)\n",
        "    doc = nlp(text)\n",
        "\n",
        "    tokens = [token for token in doc if not token.is_space and not token.is_punct]\n",
        "    word_count = len(tokens)\n",
        "    char_count = sum(len(t.text) for t in tokens)\n",
        "\n",
        "    unique_tokens = set(t.text for t in tokens)\n",
        "    ttr = len(unique_tokens) / word_count if word_count > 0 else 0.0\n",
        "\n",
        "    personal_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"}\n",
        "    pp_count = sum(1 for t in tokens if t.text.lower() in personal_pronouns)\n",
        "\n",
        "    sentiment_scores = sid.polarity_scores(text)\n",
        "    polarity = sentiment_scores[\"compound\"]\n",
        "\n",
        "    pos_counts = Counter(t.pos_ for t in tokens)\n",
        "    total_tokens_for_pos = sum(pos_counts.values())\n",
        "    pos_ratios = {pos.lower(): count / total_tokens_for_pos for pos, count in pos_counts.items()}\n",
        "\n",
        "    tokens_tags = [(token.text.lower(), token.tag_) for token in tokens]\n",
        "    past_verbs = sum(1 for word, tag in tokens_tags if tag in [\"VBD\", \"VBN\"])\n",
        "    present_verbs = sum(1 for word, tag in tokens_tags if tag in [\"VB\", \"VBP\", \"VBZ\"])\n",
        "    future_verbs = sum(1 for i, (word, tag) in enumerate(tokens_tags) if word in [\"will\", \"shall\"] and i + 1 < len(tokens_tags) and tokens_tags[i + 1][1] == \"VB\")\n",
        "\n",
        "    total_verbs = max(1, past_verbs + present_verbs + future_verbs)\n",
        "    tense_past_ratio = past_verbs / total_verbs\n",
        "    tense_present_ratio = present_verbs / total_verbs\n",
        "    tense_future_ratio = future_verbs / total_verbs\n",
        "\n",
        "    features = {\n",
        "        \"word_count\": word_count,\n",
        "        \"char_count\": char_count,\n",
        "        \"type_token_ratio\": ttr,\n",
        "        \"personal_pronoun_count\": pp_count,\n",
        "        \"sentiment_polarity\": polarity,\n",
        "        \"tense_past_ratio\": tense_past_ratio,\n",
        "        \"tense_present_ratio\": tense_present_ratio,\n",
        "        \"tense_future_ratio\": tense_future_ratio,\n",
        "    }\n",
        "\n",
        "    for pos_tag in [\"adv\", \"pron\", \"verb\", \"noun\"]: # Mistake of not including all POS tags\n",
        "        features[f\"pos_{pos_tag}_ratio\"] = pos_ratios.get(pos_tag, 0.0)\n",
        "\n",
        "    return features\n",
        "\n",
        "# --- Feature Extraction ---\n",
        "feature_rows = []\n",
        "for _, row in tqdm(df_real.iterrows(), total=len(df_real), desc=\"Extracting Features\"):\n",
        "    text = row['TEXT']\n",
        "    feats = extract_features(text)\n",
        "    feats_row = {\n",
        "        \"#AUTHID\": row['#AUTHID'],\n",
        "        \"Chunk Number\": row['Chunk Number'],\n",
        "        \"cEXT\": row['cEXT'],\n",
        "        \"cNEU\": row['cNEU'],\n",
        "        \"cAGR\": row['cAGR'],\n",
        "        \"cCON\": row['cCON'],\n",
        "        \"cOPN\": row['cOPN'],\n",
        "    }\n",
        "    feats_row.update(feats)\n",
        "    feature_rows.append(feats_row)\n",
        "\n",
        "# --- Convert to DataFrame ---\n",
        "df_features = pd.DataFrame(feature_rows)\n",
        "\n",
        "# --- Post-processing / Normalization ---\n",
        "df_features['personal_pronoun_percentage'] = np.where(\n",
        "    df_features['word_count'] > 0,\n",
        "    (df_features['personal_pronoun_count'] / df_features['word_count']) * 100,\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "for col in ['pos_adv_ratio', 'pos_pron_ratio', 'pos_verb_ratio', 'pos_noun_ratio']:\n",
        "    df_features[col.replace('_ratio', '_pct')] = df_features[col] * 100\n",
        "\n",
        "for col in ['tense_past_ratio', 'tense_present_ratio', 'tense_future_ratio']:\n",
        "    df_features[col.replace('_ratio', '_pct')] = df_features[col] * 100\n",
        "\n",
        "# --- Formatting ---\n",
        "def format_features(row):\n",
        "    return (\n",
        "        f\"Word Count: {row['word_count']}, \"\n",
        "        f\"Char Count: {row['char_count']}, \"\n",
        "        f\"Lexical Diversity (TTR): {row['type_token_ratio']:.2f}, \"\n",
        "        f\"Personal Pronouns (as percentage of words): {row['personal_pronoun_percentage']:.1f}%, \"\n",
        "        f\"Sentiment Polarity Score (VADER): {row['sentiment_polarity']:.3f}, \"\n",
        "        f\"POS Distribution (as percentage of all tokens): [Adv: {row['pos_adv_pct']:.1f}%, Pron: {row['pos_pron_pct']:.1f}%, \"\n",
        "        f\"Verb: {row['pos_verb_pct']:.1f}%, Noun: {row['pos_noun_pct']:.1f}%], \"\n",
        "        f\"Tense Distribution (derived from verb tags): [Past: {row['tense_past_pct']:.1f}%, Present: {row['tense_present_pct']:.1f}%, Future: {row['tense_future_pct']:.1f}%]\"\n",
        "    )\n",
        "\n",
        "df_features['features_text'] = df_features.apply(format_features, axis=1)\n",
        "\n",
        "# --- Export to JSON ---\n",
        "export_path = \"/path/to/full_programmatic_features_extracted.json\"\n",
        "records = df_features[['#AUTHID', 'Chunk Number', 'features_text']].to_dict(orient='records')\n",
        "with open(export_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Features exported successfully to {export_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mwvnYmlLBqXc",
        "mbTur3mABREg",
        "U_8DpLQ9GINB",
        "LgAh8G3mvwtz"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
