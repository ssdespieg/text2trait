{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtKoXV4-DWjw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# WSL path to your project directory\n",
        "wsl_path = \"/mnt/g/My Drive/Sophia/MSC thesis\"\n",
        "\n",
        "# Change current working directory to the WSL path\n",
        "os.chdir(wsl_path)\n",
        "\n",
        "# Print the current working directory to verify the change\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import openai\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "###############################################################################\n",
        "# CONFIGURE OPENAI CLIENT FOR V1.0+ USAGE\n",
        "###############################################################################\n",
        "\n",
        "# API key:\n",
        "openai.api_key = \"OPENAI-API-KEY\"  # Replace with actual API key\n",
        "\n",
        "# Model name\n",
        "MODEL_NAME = \"gpt-4o-2024-11-20\"\n",
        "\n",
        "###############################################################################\n",
        "# FEATURE LIST AND PROMPT STRUCTURE\n",
        "###############################################################################\n",
        "\n",
        "FEATURE_LIST = [\n",
        "    \"Cognitive Flexibility\",\n",
        "    \"Narrative and Discourse Coherence\",\n",
        "    \"Emotional Tone\",\n",
        "    \"Self-Reflection Depth\",\n",
        "    \"Analytical Thinking\",\n",
        "]\n",
        "\n",
        "def build_prompt_for_all_features(text_chunk: str) -> str:\n",
        "    \"\"\"\n",
        "    Builds a prompt to classify all psycholinguistic features from the text chunk.\n",
        "    \"\"\"\n",
        "    features_str = \"\\n\".join(f\"- {f}\" for f in FEATURE_LIST)\n",
        "    sanitized_text_chunk = text_chunk.replace(\"{\", \"{{\").replace(\"}\", \"}}\").strip()\n",
        "\n",
        "    prompt = f\"\"\"CoT Semantic Feature Extraction Prompt\n",
        "\n",
        "You are an intelligent and disciplined assistant trained to determine the presence and degree of various nuanced psycholinguistic features in a stream-of-consciousness text. The features are:\n",
        "{features_str}.\n",
        "\n",
        "*Your task is to reason through each feature step-by-step, explaining how the text provides evidence (or lack thereof) for each feature before determining the final result and confidence score.*\n",
        "\n",
        "*Your output must be valid JSON* with the structure:\n",
        "\n",
        "{{\n",
        "  \"features\": [\n",
        "    {{\n",
        "      \"feature\": \"Cognitive Flexibility\",\n",
        "      \"reasoning_steps\": [\n",
        "        \"...\"\n",
        "      ],\n",
        "      \"result\": \"...\",\n",
        "      \"result_justification\": \"...\",\n",
        "      \"confidence_score\": 0.0,\n",
        "      \"confidence_score_justification\": \"...\"\n",
        "    }},\n",
        "    {{\n",
        "      \"feature\": \"Narrative and Discourse Coherence\",\n",
        "      \"reasoning_steps\": [\n",
        "        \"...\"\n",
        "      ],\n",
        "      \"result\": \"...\",\n",
        "      \"result_justification\": \"...\",\n",
        "      \"confidence_score\": 0.0,\n",
        "      \"confidence_score_justification\": \"...\"\n",
        "    }},\n",
        "    ...\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Rules:\n",
        "1. For each feature, reason step-by-step as described, then provide the final evaluation in the JSON format.\n",
        "2. If insufficient data, set 'result'='Not able to evaluate', 'result_justification'='Not able to evaluate', 'confidence_score'=0.0, 'confidence_score_justification'='Not able to evaluate'.\n",
        "3. No extra text, no code fences, no keys beyond what's shown.\n",
        "4. Provide detailed justifications for the reasoning, results, and confidence scores.\n",
        "\n",
        "The text: {sanitized_text_chunk}\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def classify_text_chunk(text_chunk: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls the OpenAI API to classify features from the given text chunk.\n",
        "    \"\"\"\n",
        "    user_prompt = build_prompt_for_all_features(text_chunk)\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful, disciplined assistant that outputs JSON only.\"},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=1.0,\n",
        "        max_tokens=1500\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "###############################################################################\n",
        "# MAIN LOGIC: READ LINES, CALL MODEL, SAVE RESULTS INCREMENTALLY\n",
        "###############################################################################\n",
        "\n",
        "def main():\n",
        "    input_json = \"full_chunked_local_minima_pass_2_0.40.json\"\n",
        "    output_file = \"cot-structured_feature_extraction_openai_gpt-4o_temp_1.0.json\"\n",
        "    total_lines_to_process = 2000\n",
        "\n",
        "    if not os.path.exists(input_json):\n",
        "        print(f\"[ERROR] Input file does not exist: {input_json}\")\n",
        "        return\n",
        "\n",
        "    # Check previously processed lines\n",
        "    processed_count = 0\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            processed_count = sum(1 for _ in f)\n",
        "        print(f\"[INFO] Found {processed_count} lines already processed.\")\n",
        "\n",
        "    # Process lines and save results incrementally\n",
        "    with open(output_file, \"a\", encoding=\"utf-8\") as out_f:\n",
        "        with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            with tqdm(total=total_lines_to_process, desc=\"Processing Chunks\", initial=processed_count) as pbar:\n",
        "                for i, line in enumerate(f):\n",
        "                    if i >= processed_count:\n",
        "                        row = json.loads(line)\n",
        "                        author_id = row.get(\"#AUTHID\", \"\")\n",
        "                        chunk_number = row.get(\"Chunk Number\", \"\")\n",
        "                        text_chunk = row.get(\"TEXT\", \"\")\n",
        "\n",
        "                        print(f\"\\n[PROCESSING] Row {i} => Author: {author_id}, Chunk: {chunk_number}\")\n",
        "\n",
        "                        try:\n",
        "                            raw_json = classify_text_chunk(text_chunk)\n",
        "                            print(\"[RAW MODEL OUTPUT]\\n\", raw_json)\n",
        "\n",
        "                            try:\n",
        "                                parsed = json.loads(raw_json)\n",
        "                            except json.JSONDecodeError as e:\n",
        "                                print(f\"[ERROR] Could not parse JSON for row {i}: {e}\")\n",
        "                                parsed = {\"error\": \"Invalid JSON\", \"exception\": str(e)}\n",
        "\n",
        "                            record = {\n",
        "                                \"author_id\": author_id,\n",
        "                                \"chunk_number\": chunk_number,\n",
        "                                \"model_output\": parsed\n",
        "                            }\n",
        "                            out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "                            out_f.flush()\n",
        "                            processed_count += 1\n",
        "                            pbar.update(1)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERROR] In call to model: {e}\")\n",
        "                            record = {\n",
        "                                \"author_id\": author_id,\n",
        "                                \"chunk_number\": chunk_number,\n",
        "                                \"model_output\": {\"error\": str(e)}\n",
        "                            }\n",
        "                            out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "                            out_f.flush()\n",
        "\n",
        "                    if processed_count >= total_lines_to_process:\n",
        "                        break\n",
        "\n",
        "    print(f\"\\n[DONE] Wrote results to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "xUwuP3ljnl24"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}